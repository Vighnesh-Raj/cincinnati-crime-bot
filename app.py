# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ckeG658RrRAlBBX-iC1Ap_cnRSoyDSo8
"""

# app.py
import os
os.environ["TRANSFORMERS_NO_TF"] = "1"

import pandas as pd
import time
from transformers import pipeline
from huggingface_hub import hf_hub_download
import streamlit as st

# Load data from Hugging Face
@st.cache_data(show_spinner=False)
def load_crime_data():
    csv_path = hf_hub_download(
        repo_id="mlsystemsg1/cincinnati-crime-data",
        repo_type="dataset",
        filename="calls_for_service_latest.csv"
    )
    df = pd.read_csv(csv_path, low_memory=False)
    df.columns = [col.lower() for col in df.columns]
    return df

# Filter rows based on user query
def get_relevant_rows(question, df, num_rows=10):
    q = question.lower()
    filtered = df.copy()

    if 'sna_neighborhood' in df.columns:
        for neighborhood in df['sna_neighborhood'].dropna().unique():
            if neighborhood.lower() in q:
                filtered = filtered[filtered['sna_neighborhood'].str.contains(neighborhood, case=False, na=False)]
                break

    if 'incident_type_desc' in df.columns:
        for offense in df['incident_type_desc'].dropna().unique():
            if offense.lower() in q:
                filtered = filtered[filtered['incident_type_desc'].str.contains(offense, case=False, na=False)]
                break

    if 'create_time_incident' in df.columns and any(word in q for word in ["last", "latest", "recent"]):
        filtered = filtered.sort_values(by='create_time_incident', ascending=False)

    return filtered.head(min(num_rows, len(filtered)))

# Generate summary of filtered data
def generate_summary(question, filtered_df):
    q = question.lower()

    if filtered_df.empty:
        return "No matching records found."

    if any(word in q for word in ["how many", "number of", "count"]):
        return f"{len(filtered_df)} incidents matched your query."

    if "most common" in q and 'incident_type_desc' in filtered_df.columns:
        most_common = filtered_df['incident_type_desc'].value_counts().idxmax()
        count = filtered_df['incident_type_desc'].value_counts().max()
        return f"The most common crime is {most_common} with {count} incidents."

    examples = []
    for _, row in filtered_df.iterrows():
        try:
            date = row.get('create_time_incident', 'N/A')
            offense = row.get('incident_type_desc', 'N/A')
            neighborhood = row.get('sna_neighborhood', 'N/A')
            incident = row.get('event_number', 'N/A')
            examples.append(f"On {date}, a {offense} occurred in {neighborhood} (Incident #{incident}).")
        except:
            continue
    return "\n".join(examples[:10])

# Call the LLM
def answer_with_llm(question, data_rows, model, model_name="google/flan-t5-base", prompt_version="v3"):
    if data_rows.empty:
        return "Sorry, I couldn't find any data matching that question."

    context = generate_summary(question, data_rows)
    prompt = f"""
You are a helpful assistant analyzing Cincinnati crime data.

Here are some relevant data points:
{context}

Now answer this question based on the above:
{question}
    """.strip()

    try:
        result = model(prompt, max_new_tokens=150)[0]['generated_text']
    except Exception as e:
        return f"An error occurred while generating the answer: {str(e)}"

    return result.strip()

# Streamlit App
if __name__ == "__main__":
    st.title("Cincinnati Crime Chatbot")
    st.markdown("""
    üí° **How to use this app:**
    - Enter a question about crime in Cincinnati in the input box below.
    - Select a model from the dropdown (default is FLAN-T5).
    - The chatbot will process recent incident data and respond with an answer.
    - You can explore the sample data used below the response.
    """)

    question = st.text_input("Ask a question about crime in Cincinnati:")

    model_name = st.selectbox("Choose a model:", ["google/flan-t5-base"])
    if model_name:
        with st.spinner("Loading model..."):
            llm_model = pipeline("text2text-generation", model=model_name)

    df = load_crime_data()

    if question:
        relevant_data = get_relevant_rows(question, df)
        with st.spinner("Generating answer..."):
            answer = answer_with_llm(question, relevant_data, llm_model, model_name)
        st.markdown("### ü§ñ Answer")
        st.write(answer)
        st.markdown("---")
        st.markdown("### üîç Sample Data Used")
        st.dataframe(relevant_data.head(10))